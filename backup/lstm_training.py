#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MPTCP SDN LSTMæ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨ç”¨äºè®­ç»ƒLSTMç¥ç»ç½‘ç»œæ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import logging
import time
import os
from dataclasses import dataclass
import pickle

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class NetworkMetrics:
    """ç½‘ç»œæ€§èƒ½æŒ‡æ ‡"""
    bandwidth_utilization: float
    latency: float
    packet_loss_rate: float
    throughput: float
    congestion_window_size: int
    rtt: float
    subflow_count: int
    path_diversity: float

class LSTMNetworkPredictor(nn.Module):
    """LSTMç½‘ç»œæ€§èƒ½é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size=8, hidden_size=64, num_layers=2, output_size=4):
        super(LSTMNetworkPredictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_size, 32)
        self.fc2 = nn.Linear(32, output_size)
        self.dropout = nn.Dropout(0.3)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc1(lstm_out[:, -1, :])
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        
        return self.sigmoid(out)

class PathSelectionLSTM(nn.Module):
    """è·¯å¾„é€‰æ‹©LSTMæ¨¡å‹"""
    
    def __init__(self, input_size=8, hidden_size=32, num_layers=1, num_paths=8):
        super(PathSelectionLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_paths)
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        path_scores = self.fc(lstm_out[:, -1, :])
        return self.softmax(path_scores)

class CongestionPredictionLSTM(nn.Module):
    """æ‹¥å¡é¢„æµ‹LSTMæ¨¡å‹"""
    
    def __init__(self, input_size=8, hidden_size=24, num_layers=1):
        super(CongestionPredictionLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        congestion_prob = self.fc(lstm_out[:, -1, :])
        return self.sigmoid(congestion_prob)

class NetworkDataGenerator:
    """ç½‘ç»œæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.rng = np.random.RandomState(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°
        
    def generate_synthetic_data(self, num_samples=1000, sequence_length=10):
        """ç”Ÿæˆåˆæˆç½‘ç»œæ•°æ®"""
        logger.info(f"ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦: {sequence_length}")
        
        X, y_performance, y_congestion, y_paths = [], [], [], []
        
        for i in range(num_samples):
            # ç”Ÿæˆä¸€ä¸ªåºåˆ—çš„ç½‘ç»œçŠ¶æ€
            sequence = []
            for t in range(sequence_length):
                # æ¨¡æ‹Ÿæ—¶é—´ç›¸å…³çš„ç½‘ç»œçŠ¶æ€å˜åŒ–
                base_utilization = 0.5 + 0.3 * np.sin(2 * np.pi * t / 10)
                noise = self.rng.normal(0, 0.1)
                
                metrics = [
                    min(1.0, max(0.0, base_utilization + noise)),  # bandwidth_utilization
                    self.rng.uniform(5, 50),  # latency
                    self.rng.exponential(0.01),  # packet_loss_rate
                    self.rng.uniform(100, 1000),  # throughput
                    self.rng.uniform(0.1, 1.0),  # congestion_window_size (normalized)
                    self.rng.uniform(10, 200),  # rtt
                    self.rng.randint(1, 9),  # subflow_count
                    self.rng.uniform(0.5, 1.0)  # path_diversity
                ]
                
                # å½’ä¸€åŒ–å¤„ç†
                normalized_metrics = [
                    metrics[0],  # bandwidth_utilization å·²å½’ä¸€åŒ–
                    metrics[1] / 100.0,  # latency å½’ä¸€åŒ–
                    min(1.0, metrics[2]),  # packet_loss_rate
                    metrics[3] / 1000.0,  # throughput å½’ä¸€åŒ–
                    metrics[4],  # congestion_window_size å·²å½’ä¸€åŒ–
                    metrics[5] / 200.0,  # rtt å½’ä¸€åŒ–
                    metrics[6] / 8.0,  # subflow_count å½’ä¸€åŒ–
                    metrics[7]  # path_diversity å·²å½’ä¸€åŒ–
                ]
                
                sequence.append(normalized_metrics)
            
            X.append(sequence)
            
            # ç”Ÿæˆç›®æ ‡æ ‡ç­¾
            last_metrics = sequence[-1]
            
            # æ€§èƒ½é¢„æµ‹ç›®æ ‡: [å¸¦å®½åˆ©ç”¨ç‡, å»¶è¿Ÿ, ååé‡, è´¨é‡åˆ†æ•°]
            quality_score = (last_metrics[0] * last_metrics[3] * 
                            (1 - last_metrics[2]) * (1 - last_metrics[1]))
            performance_target = [
                last_metrics[0],  # å¸¦å®½åˆ©ç”¨ç‡
                last_metrics[1],  # å»¶è¿Ÿ
                last_metrics[3],  # ååé‡
                quality_score     # æ•´ä½“è´¨é‡åˆ†æ•°
            ]
            
            # æ‹¥å¡é¢„æµ‹ç›®æ ‡
            congestion_prob = 1.0 if last_metrics[2] > 0.02 else 0.0
            
            # è·¯å¾„é€‰æ‹©ç›®æ ‡ (åŸºäºæ€§èƒ½é€‰æ‹©æœ€ä¼˜è·¯å¾„)
            path_target = min(7, int(quality_score * 8))
            
            y_performance.append(performance_target)
            y_congestion.append([congestion_prob])
            y_paths.append(path_target)
            
            if (i + 1) % 100 == 0:
                logger.info(f"å·²ç”Ÿæˆ {i + 1}/{num_samples} ä¸ªæ ·æœ¬")
        
        return (np.array(X, dtype=np.float32), 
                np.array(y_performance, dtype=np.float32),
                np.array(y_congestion, dtype=np.float32), 
                np.array(y_paths, dtype=np.int64))

class LSTMTrainer:
    """LSTMæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, device='cpu'):
        self.device = device
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.performance_model = LSTMNetworkPredictor().to(device)
        self.path_model = PathSelectionLSTM().to(device)
        self.congestion_model = CongestionPredictionLSTM().to(device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.performance_optimizer = optim.Adam(self.performance_model.parameters(), lr=0.001)
        self.path_optimizer = optim.Adam(self.path_model.parameters(), lr=0.001)
        self.congestion_optimizer = optim.Adam(self.congestion_model.parameters(), lr=0.001)
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'performance_loss': [],
            'path_loss': [],
            'congestion_loss': []
        }
    
    def train_models(self, X, y_performance, y_congestion, y_paths, 
                    epochs=100, batch_size=32, validation_split=0.2):
        """è®­ç»ƒæ‰€æœ‰LSTMæ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        
        # æ•°æ®è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_perf_tensor = torch.FloatTensor(y_performance).to(self.device)
        y_cong_tensor = torch.FloatTensor(y_congestion).to(self.device)
        y_path_tensor = torch.LongTensor(y_paths).to(self.device)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        num_samples = len(X)
        val_size = int(num_samples * validation_split)
        train_size = num_samples - val_size
        
        indices = torch.randperm(num_samples)
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]
        
        # è®­ç»ƒæ•°æ®
        X_train = X_tensor[train_indices]
        y_perf_train = y_perf_tensor[train_indices]
        y_cong_train = y_cong_tensor[train_indices]
        y_path_train = y_path_tensor[train_indices]
        
        # éªŒè¯æ•°æ®
        X_val = X_tensor[val_indices]
        y_perf_val = y_perf_tensor[val_indices]
        y_cong_val = y_cong_tensor[val_indices]
        y_path_val = y_path_tensor[val_indices]
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {train_size}, éªŒè¯é›†å¤§å°: {val_size}")
        
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_losses = self._train_epoch(
                X_train, y_perf_train, y_cong_train, y_path_train, batch_size
            )
            
            # éªŒè¯é˜¶æ®µ
            val_losses = self._validate_epoch(
                X_val, y_perf_val, y_cong_val, y_path_val
            )
            
            # è®°å½•æŸå¤±
            for key in train_losses:
                self.train_history[key].append(train_losses[key])
            
            # æ—©åœæ£€æŸ¥
            current_val_loss = sum(val_losses.values())
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_models('best_models')
            else:
                patience_counter += 1
            
            if epoch % 10 == 0 or epoch == epochs - 1:
                logger.info(f"Epoch {epoch+1}/{epochs}")
                logger.info(f"  è®­ç»ƒæŸå¤± - æ€§èƒ½: {train_losses['performance_loss']:.4f}, "
                          f"è·¯å¾„: {train_losses['path_loss']:.4f}, "
                          f"æ‹¥å¡: {train_losses['congestion_loss']:.4f}")
                logger.info(f"  éªŒè¯æŸå¤± - æ€§èƒ½: {val_losses['performance_loss']:.4f}, "
                          f"è·¯å¾„: {val_losses['path_loss']:.4f}, "
                          f"æ‹¥å¡: {val_losses['congestion_loss']:.4f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                logger.info(f"æ—©åœäºç¬¬ {epoch+1} è½®ï¼ŒéªŒè¯æŸå¤±æœªæ”¹å–„")
                break
        
        logger.info("è®­ç»ƒå®Œæˆ!")
        return self.train_history
    
    def _train_epoch(self, X, y_perf, y_cong, y_path, batch_size):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.performance_model.train()
        self.path_model.train()
        self.congestion_model.train()
        
        total_losses = {'performance_loss': 0, 'path_loss': 0, 'congestion_loss': 0}
        num_batches = (len(X) + batch_size - 1) // batch_size
        
        for i in range(0, len(X), batch_size):
            end_idx = min(i + batch_size, len(X))
            batch_X = X[i:end_idx]
            batch_y_perf = y_perf[i:end_idx]
            batch_y_cong = y_cong[i:end_idx]
            batch_y_path = y_path[i:end_idx]
            
            # æ€§èƒ½é¢„æµ‹æ¨¡å‹è®­ç»ƒ
            self.performance_optimizer.zero_grad()
            perf_pred = self.performance_model(batch_X)
            perf_loss = self.mse_loss(perf_pred, batch_y_perf)
            perf_loss.backward()
            self.performance_optimizer.step()
            
            # æ‹¥å¡é¢„æµ‹æ¨¡å‹è®­ç»ƒ
            self.congestion_optimizer.zero_grad()
            cong_pred = self.congestion_model(batch_X)
            cong_loss = self.bce_loss(cong_pred, batch_y_cong)
            cong_loss.backward()
            self.congestion_optimizer.step()
            
            # è·¯å¾„é€‰æ‹©æ¨¡å‹è®­ç»ƒ
            self.path_optimizer.zero_grad()
            path_pred = self.path_model(batch_X)
            path_loss = self.ce_loss(path_pred, batch_y_path)
            path_loss.backward()
            self.path_optimizer.step()
            
            total_losses['performance_loss'] += perf_loss.item()
            total_losses['path_loss'] += path_loss.item()
            total_losses['congestion_loss'] += cong_loss.item()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in total_losses:
            total_losses[key] /= num_batches
        
        return total_losses
    
    def _validate_epoch(self, X, y_perf, y_cong, y_path):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.performance_model.eval()
        self.path_model.eval()
        self.congestion_model.eval()
        
        with torch.no_grad():
            perf_pred = self.performance_model(X)
            cong_pred = self.congestion_model(X)
            path_pred = self.path_model(X)
            
            perf_loss = self.mse_loss(perf_pred, y_perf).item()
            cong_loss = self.bce_loss(cong_pred, y_cong).item()
            path_loss = self.ce_loss(path_pred, y_path).item()
        
        return {
            'performance_loss': perf_loss,
            'path_loss': path_loss,
            'congestion_loss': cong_loss
        }
    
    def save_models(self, save_dir='trained_models'):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        
        torch.save(self.performance_model.state_dict(), 
                  os.path.join(save_dir, 'performance_lstm.pth'))
        torch.save(self.path_model.state_dict(), 
                  os.path.join(save_dir, 'path_selection_lstm.pth'))
        torch.save(self.congestion_model.state_dict(), 
                  os.path.join(save_dir, 'congestion_prediction_lstm.pth'))
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(os.path.join(save_dir, 'training_history.pkl'), 'wb') as f:
            pickle.dump(self.train_history, f)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")
    
    def load_models(self, load_dir='trained_models'):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            self.performance_model.load_state_dict(
                torch.load(os.path.join(load_dir, 'performance_lstm.pth'), 
                          map_location=self.device))
            self.path_model.load_state_dict(
                torch.load(os.path.join(load_dir, 'path_selection_lstm.pth'), 
                          map_location=self.device))
            self.congestion_model.load_state_dict(
                torch.load(os.path.join(load_dir, 'congestion_prediction_lstm.pth'), 
                          map_location=self.device))
            
            logger.info(f"æ¨¡å‹å·²ä» {load_dir} åŠ è½½")
            return True
        except FileNotFoundError:
            logger.warning(f"æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹åœ¨ {load_dir}")
            return False
    
    def plot_training_history(self, save_path='training_history.png'):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # æ€§èƒ½æŸå¤±
        axes[0].plot(self.train_history['performance_loss'])
        axes[0].set_title('æ€§èƒ½é¢„æµ‹æŸå¤±')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('MSE Loss')
        axes[0].grid(True)
        
        # è·¯å¾„é€‰æ‹©æŸå¤±
        axes[1].plot(self.train_history['path_loss'])
        axes[1].set_title('è·¯å¾„é€‰æ‹©æŸå¤±')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('CrossEntropy Loss')
        axes[1].grid(True)
        
        # æ‹¥å¡é¢„æµ‹æŸå¤±
        axes[2].plot(self.train_history['congestion_loss'])
        axes[2].set_title('æ‹¥å¡é¢„æµ‹æŸå¤±')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('BCE Loss')
        axes[2].grid(True)
        
        plt.tight_layout()
        plt.savefig(save_path)
        plt.show()
        logger.info(f"è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ° {save_path}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 70)
    print("ğŸ§  MPTCP SDN LSTMæ¨¡å‹è®­ç»ƒ")
    print("ç‰ˆæœ¬: 1.0")
    print("=" * 70)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    data_generator = NetworkDataGenerator()
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    X, y_performance, y_congestion, y_paths = data_generator.generate_synthetic_data(
        num_samples=2000, sequence_length=15
    )
    
    print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ:")
    print(f"   è¾“å…¥å½¢çŠ¶: {X.shape}")
    print(f"   æ€§èƒ½ç›®æ ‡å½¢çŠ¶: {y_performance.shape}")
    print(f"   æ‹¥å¡ç›®æ ‡å½¢çŠ¶: {y_congestion.shape}")
    print(f"   è·¯å¾„ç›®æ ‡å½¢çŠ¶: {y_paths.shape}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LSTMTrainer(device=device)
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
    start_time = time.time()
    
    training_history = trainer.train_models(
        X, y_performance, y_congestion, y_paths,
        epochs=50, batch_size=64, validation_split=0.2
    )
    
    training_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f} ç§’")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_models('trained_models')
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_training_history()
    
    # æ¨¡å‹è¯„ä¼°
    print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°:")
    final_losses = {
        'performance_loss': training_history['performance_loss'][-1],
        'path_loss': training_history['path_loss'][-1],
        'congestion_loss': training_history['congestion_loss'][-1]
    }
    
    for loss_name, loss_value in final_losses.items():
        print(f"   {loss_name}: {loss_value:.4f}")
    
    print("\nğŸ¯ è®­ç»ƒæ€»ç»“:")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(X)}")
    print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: trained_models/")
    print("   å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç½‘ç»œä¼˜åŒ–é¢„æµ‹")

if __name__ == "__main__":
    main() 