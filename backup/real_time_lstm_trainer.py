#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ”„ å®æ—¶LSTMæ¨¡å‹è®­ç»ƒå™¨
æ”¯æŒåœ¨çº¿å­¦ä¹ å’Œå¢é‡è®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®å®æ—¶ç½‘ç»œæ•°æ®åŠ¨æ€æ›´æ–°æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
import time
from collections import deque
from datetime import datetime
import json
import threading
import queue

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OnlineLSTMPredictor(nn.Module):
    """åœ¨çº¿LSTMé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size=8, hidden_size=64, num_layers=2, output_size=1):
        super(OnlineLSTMPredictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return self.sigmoid(out)

class RealTimeTrainer:
    """å®æ—¶LSTMè®­ç»ƒå™¨"""
    
    def __init__(self, model_path=None, learning_rate=0.001, 
                 buffer_size=1000, batch_size=32, update_frequency=10):
        self.model = OnlineLSTMPredictor()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
        
        # åœ¨çº¿å­¦ä¹ å‚æ•°
        self.buffer_size = buffer_size
        self.batch_size = batch_size
        self.update_frequency = update_frequency
        
        # æ•°æ®ç¼“å†²åŒº
        self.data_buffer = deque(maxlen=buffer_size)
        self.label_buffer = deque(maxlen=buffer_size)
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'total_updates': 0,
            'last_loss': 0.0,
            'avg_loss': 0.0,
            'learning_rate': learning_rate,
            'samples_processed': 0
        }
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and self._load_model(model_path):
            logger.info("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        else:
            logger.info("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
        
        # çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—
        self.data_queue = queue.Queue()
        self.is_training = False
        self.training_thread = None
    
    def _load_model(self, model_path):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            self.model.load_state_dict(torch.load(model_path, map_location='cpu'))
            return True
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def save_model(self, path='real_time_model.pth'):
        """ä¿å­˜å½“å‰æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_stats': self.training_stats
        }, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def add_training_sample(self, network_data, performance_target):
        """æ·»åŠ è®­ç»ƒæ ·æœ¬åˆ°ç¼“å†²åŒº"""
        # å½’ä¸€åŒ–ç½‘ç»œæ•°æ®
        normalized_data = self._normalize_network_data(network_data)
        
        self.data_buffer.append(normalized_data)
        self.label_buffer.append(performance_target)
        self.training_stats['samples_processed'] += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘è®­ç»ƒ
        if len(self.data_buffer) >= self.batch_size:
            if self.training_stats['samples_processed'] % self.update_frequency == 0:
                self._trigger_incremental_training()
    
    def _normalize_network_data(self, data):
        """å½’ä¸€åŒ–ç½‘ç»œæ•°æ®"""
        # å‡è®¾è¾“å…¥æ ¼å¼: [bandwidth, latency, packet_loss, throughput, cwnd, rtt, subflows, diversity]
        normalized = [
            min(1.0, max(0.0, data[0] / 100.0)),  # bandwidth (Mbps)
            min(1.0, data[1] / 100.0),            # latency (ms)
            min(1.0, data[2]),                    # packet_loss
            min(1.0, data[3] / 1000.0),           # throughput (Mbps)
            min(1.0, max(0.0, data[4] / 100.0)), # congestion window
            min(1.0, data[5] / 200.0),            # rtt (ms)
            min(1.0, data[6] / 8.0),              # subflows
            min(1.0, max(0.0, data[7]))          # path diversity
        ]
        return normalized
    
    def _trigger_incremental_training(self):
        """è§¦å‘å¢é‡è®­ç»ƒ"""
        if not self.is_training and len(self.data_buffer) >= self.batch_size:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            train_data = list(self.data_buffer)[-self.batch_size:]
            train_labels = list(self.label_buffer)[-self.batch_size:]
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œé¿å…é˜»å¡ä¸»ç¨‹åº
            self.training_thread = threading.Thread(
                target=self._incremental_train,
                args=(train_data, train_labels)
            )
            self.training_thread.start()
    
    def _incremental_train(self, train_data, train_labels):
        """æ‰§è¡Œå¢é‡è®­ç»ƒ"""
        self.is_training = True
        
        try:
            # è½¬æ¢ä¸ºå¼ é‡
            X = torch.FloatTensor(train_data).unsqueeze(0)  # æ·»åŠ åºåˆ—ç»´åº¦
            y = torch.FloatTensor([[label] for label in train_labels])
            
            # è®­ç»ƒæ­¥éª¤
            self.model.train()
            self.optimizer.zero_grad()
            
            predictions = self.model(X)
            loss = self.criterion(predictions, y)
            
            loss.backward()
            self.optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            self.training_stats['total_updates'] += 1
            self.training_stats['last_loss'] = loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            if self.training_stats['total_updates'] == 1:
                self.training_stats['avg_loss'] = loss.item()
            else:
                alpha = 0.1  # æŒ‡æ•°ç§»åŠ¨å¹³å‡
                self.training_stats['avg_loss'] = (
                    alpha * loss.item() + 
                    (1 - alpha) * self.training_stats['avg_loss']
                )
            
            logger.info(f"ğŸ”„ å¢é‡è®­ç»ƒå®Œæˆ - æŸå¤±: {loss.item():.4f}, "
                       f"å¹³å‡æŸå¤±: {self.training_stats['avg_loss']:.4f}")
            
        except Exception as e:
            logger.error(f"å¢é‡è®­ç»ƒå¤±è´¥: {e}")
        
        finally:
            self.is_training = False
    
    def predict(self, network_data):
        """è¿›è¡Œé¢„æµ‹"""
        self.model.eval()
        
        normalized_data = self._normalize_network_data(network_data)
        
        with torch.no_grad():
            # å°†æ•°æ®è½¬æ¢ä¸ºåºåˆ—æ ¼å¼
            X = torch.FloatTensor([normalized_data]).unsqueeze(0)
            prediction = self.model(X).item()
        
        return prediction
    
    def get_training_stats(self):
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return self.training_stats.copy()
    
    def reset_buffer(self):
        """é‡ç½®æ•°æ®ç¼“å†²åŒº"""
        self.data_buffer.clear()
        self.label_buffer.clear()
        logger.info("æ•°æ®ç¼“å†²åŒºå·²é‡ç½®")

class NetworkSimulator:
    """ç½‘ç»œçŠ¶æ€æ¨¡æ‹Ÿå™¨"""
    
    def __init__(self):
        self.time_step = 0
        
    def generate_network_sample(self):
        """ç”Ÿæˆç½‘ç»œæ ·æœ¬"""
        self.time_step += 1
        
        # æ¨¡æ‹Ÿæ—¶é—´å˜åŒ–çš„ç½‘ç»œçŠ¶æ€
        base_load = 0.5 + 0.3 * np.sin(2 * np.pi * self.time_step / 100)
        noise = np.random.normal(0, 0.1)
        
        network_data = [
            max(0, min(100, 50 + base_load * 50 + noise * 10)),  # bandwidth
            max(1, 20 + np.random.exponential(10)),              # latency
            max(0, np.random.exponential(0.01)),                 # packet_loss
            max(0, min(1000, 500 + base_load * 300)),           # throughput
            max(0, min(100, 50 + np.random.normal(0, 10))),     # cwnd
            max(10, 50 + np.random.exponential(20)),            # rtt
            np.random.randint(1, 8),                             # subflows
            np.random.uniform(0.5, 1.0)                         # diversity
        ]
        
        # åŸºäºå½“å‰çŠ¶æ€è®¡ç®—æ€§èƒ½ç›®æ ‡
        performance = (
            (network_data[0] / 100.0) *      # å¸¦å®½åˆ©ç”¨ç‡
            (1 - network_data[2]) *          # ä½ä¸¢åŒ…ç‡
            (network_data[3] / 1000.0) *     # é«˜ååé‡
            (1 - network_data[1] / 100.0)    # ä½å»¶è¿Ÿ
        )
        
        return network_data, performance

def run_real_time_training_demo():
    """è¿è¡Œå®æ—¶è®­ç»ƒæ¼”ç¤º"""
    print("ğŸ”„ å®æ—¶LSTMè®­ç»ƒæ¼”ç¤º")
    print("="*50)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = RealTimeTrainer(
        learning_rate=0.001,
        buffer_size=500,
        batch_size=16,
        update_frequency=5
    )
    
    # åˆå§‹åŒ–ç½‘ç»œæ¨¡æ‹Ÿå™¨
    simulator = NetworkSimulator()
    
    print("ğŸš€ å¼€å§‹å®æ—¶è®­ç»ƒ...")
    print("æŒ‰ Ctrl+C åœæ­¢è®­ç»ƒ\n")
    
    try:
        for step in range(200):  # æ¨¡æ‹Ÿ200ä¸ªæ—¶é—´æ­¥
            # ç”Ÿæˆç½‘ç»œæ•°æ®
            network_data, true_performance = simulator.generate_network_sample()
            
            # ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œé¢„æµ‹
            predicted_performance = trainer.predict(network_data)
            
            # æ·»åŠ è®­ç»ƒæ ·æœ¬
            trainer.add_training_sample(network_data, true_performance)
            
            # æ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
            if step % 10 == 0:
                stats = trainer.get_training_stats()
                print(f"æ­¥éª¤ {step:3d}: "
                      f"çœŸå®å€¼={true_performance:.3f}, "
                      f"é¢„æµ‹å€¼={predicted_performance:.3f}, "
                      f"è¯¯å·®={abs(true_performance - predicted_performance):.3f}")
                print(f"         "
                      f"è®­ç»ƒæ›´æ–°={stats['total_updates']}, "
                      f"å¹³å‡æŸå¤±={stats['avg_loss']:.4f}, "
                      f"å¤„ç†æ ·æœ¬={stats['samples_processed']}")
                print()
            
            # æ¨¡æ‹Ÿå®æ—¶å¤„ç†å»¶è¿Ÿ
            time.sleep(0.1)
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ è®­ç»ƒè¢«ç”¨æˆ·åœæ­¢")
    
    # ç­‰å¾…è®­ç»ƒçº¿ç¨‹å®Œæˆ
    if trainer.training_thread and trainer.training_thread.is_alive():
        trainer.training_thread.join()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model('real_time_trained_model.pth')
    
    # ç”ŸæˆæŠ¥å‘Š
    final_stats = trainer.get_training_stats()
    
    report = {
        'training_mode': 'real_time_online',
        'timestamp': datetime.now().isoformat(),
        'final_stats': final_stats,
        'configuration': {
            'buffer_size': trainer.buffer_size,
            'batch_size': trainer.batch_size,
            'update_frequency': trainer.update_frequency
        }
    }
    
    with open('real_time_training_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print("\nğŸ“Š è®­ç»ƒå®Œæˆ!")
    print(f"   æ€»è®­ç»ƒæ›´æ–°: {final_stats['total_updates']}")
    print(f"   å¤„ç†æ ·æœ¬æ•°: {final_stats['samples_processed']}")
    print(f"   æœ€ç»ˆæŸå¤±: {final_stats['last_loss']:.4f}")
    print(f"   å¹³å‡æŸå¤±: {final_stats['avg_loss']:.4f}")
    print("   ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: real_time_training_report.json")
    print("   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: real_time_trained_model.pth")

if __name__ == "__main__":
    run_real_time_training_demo() 